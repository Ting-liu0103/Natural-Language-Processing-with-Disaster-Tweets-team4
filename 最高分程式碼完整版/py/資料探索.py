# -*- coding: utf-8 -*-
"""è³‡æ–™æ¢ç´¢.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOpm_7LTa6jloOIrmdEtv_cWgh7-2X9T
"""

#è³‡æ–™æ¢ç´¢
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
from textblob import TextBlob
import nltk
import re

# nltk è³‡æºä¸‹è¼‰
nltk.download('stopwords')
from nltk.corpus import stopwords

# ---------- 1. è¼‰å…¥è³‡æ–™ ----------
train = pd.read_csv("train.csv")

# ---------- 2. å»ºç«‹åŸºæœ¬ç‰¹å¾µ ----------
stop_words = set(stopwords.words('english'))

train['char_count'] = train['text'].apply(len)
train['word_count'] = train['text'].apply(lambda x: len(x.split()))
train['has_url'] = train['text'].str.contains('http').astype(int)
train['has_mention'] = train['text'].str.contains('@').astype(int)
train['has_hashtag'] = train['text'].str.contains('#').astype(int)
train['has_exclamation'] = train['text'].str.contains('!').astype(int)
train['has_question'] = train['text'].str.contains(r'\?').astype(int)
train['all_caps'] = train['text'].apply(lambda x: sum(1 for w in x.split() if w.isupper()))
train['text_length'] = train['text'].str.len()

# ---------- 3. åŠ å…¥ç½é›£é—œéµå­—ç‰¹å¾µ ----------
disaster_keywords = [
    'fire', 'flood', 'earthquake', 'storm', 'hurricane', 'typhoon', 'killed',
    'dead', 'died', 'explosion', 'crash', 'emergency', 'rescue', 'evacuate',
    'tsunami', 'fatal', 'injured', 'wreck'
]
train['has_disaster_keyword'] = train['text'].str.lower().apply(
    lambda x: int(any(keyword in x for keyword in disaster_keywords))
)

# ---------- 4. Sentiment ç‰¹å¾µ ----------
train['sentiment'] = train['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

# ---------- 5. å¸¸è¦‹å­—è©çµ±è¨ˆ ----------
def get_top_words(texts, n=20):
    words = ' '.join(texts).lower().split()
    words = [w for w in words if w not in stop_words and w.isalpha()]
    return Counter(words).most_common(n)

print("Top 20 words in disaster tweets:")
print(get_top_words(train[train['target'] == 1]['text']))
print("\nTop 20 words in non-disaster tweets:")
print(get_top_words(train[train['target'] == 0]['text']))

# ---------- 6. è³‡æ–™è¦–è¦ºåŒ– ----------

sns.set(style="whitegrid")

# --- (a) å­—æ•¸èˆ‡è©æ•¸ ---
plt.figure(figsize=(10,4))
sns.histplot(data=train, x='char_count', hue='target', bins=50, kde=True)
plt.title("Character Count by Class")
plt.show()

plt.figure(figsize=(10,4))
sns.histplot(data=train, x='word_count', hue='target', bins=50, kde=True)
plt.title("Word Count by Class")
plt.show()

# --- (b) WordCloud ---
def show_wordcloud(data, title):
    text = ' '.join(data)
    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

show_wordcloud(train[train['target'] == 1]['text'], "ğŸš¨ Disaster Tweets")
show_wordcloud(train[train['target'] == 0]['text'], "ğŸ“¢ Non-Disaster Tweets")

# --- (c) ç‰¹å¾µçµ±è¨ˆåœ– ---
features = ['has_url', 'has_mention', 'has_hashtag', 'has_exclamation', 'has_question']
for feature in features:
    plt.figure(figsize=(5,4))
    sns.countplot(x=feature, hue='target', data=train)
    plt.title(f"{feature} by Class")
    plt.show()

# --- (d) ALL CAPS æ•£å¸ƒ ---
plt.figure(figsize=(6,4))
sns.boxplot(x='target', y='all_caps', data=train)
plt.title("ALL CAPS Words by Class")
plt.show()

# --- (e) Sentiment ---
plt.figure(figsize=(10,4))
sns.histplot(data=train, x='sentiment', hue='target', bins=50, kde=True)
plt.title("Sentiment Polarity Distribution")
plt.show()

# --- (f) ç‰¹å¾µé–“ç›¸é—œæ€§ ---
correlation_features = [
    'char_count', 'word_count', 'all_caps', 'text_length',
    'has_url', 'has_mention', 'has_hashtag',
    'has_exclamation', 'has_question', 'has_disaster_keyword',
    'sentiment', 'target'
]
plt.figure(figsize=(12,8))
sns.heatmap(train[correlation_features].corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# å®‰è£å¿…è¦å¥—ä»¶
!pip install transformers
!pip install nltk
!pip install --upgrade sympy


# åŒ¯å…¥å‡½å¼åº«
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW

from transformers import get_scheduler
from sklearn.model_selection import train_test_split
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
# è¼‰å…¥è³‡æ–™
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
submission = pd.read_csv("sample_submission.csv")

import re
from nltk.corpus import stopwords
from collections import Counter
import nltk

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# 1. ç°¡å–® tokenize å‡½å¼
def simple_tokenize(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|@\w+|#', '', text)
    tokens = re.findall(r'\b\w+\b', text)
    return [t for t in tokens if t not in stop_words and len(t) > 2]

# 2. å¾ target=1 çš„æ–‡æœ¬ä¸­æå–ç½é›£é—œéµè©
disaster_texts = train[train['target'] == 1]['text'].tolist()
all_words = []

for text in disaster_texts:
    all_words.extend(simple_tokenize(text))

# 3. å–å¾—é«˜é »è©å‰ N å€‹ä½œç‚ºç½é›£é—œéµè©
top_n = 20
disaster_keywords = [word for word, freq in Counter(all_words).most_common(top_n)]

print("ğŸ” Top disaster keywords:", disaster_keywords)

# 4. åˆ¤æ–·æ–‡æœ¬æ˜¯å¦å«æœ‰ç½é›£é—œéµè©
def contains_disaster_keyword(text):
    text_lower = text.lower()
    return int(any(keyword in text_lower for keyword in disaster_keywords))

train['has_disaster_keyword'] = train['text'].apply(contains_disaster_keyword)
test['has_disaster_keyword'] = test['text'].apply(contains_disaster_keyword)

# 5. æ¸…ç†æ–‡å­—
def clean_text(text):
    text = re.sub(r'http\S+|www\S+', '[URL]', text)
    text = re.sub(r'@\w+', '[MENTION]', text)
    text = re.sub(r'#(\w+)', r'[HASHTAG] \1', text)
    text = re.sub(r'!', '[EXCLAMATION]', text)
    text = re.sub(r'\?', '[QUESTION]', text)
    text = text.lower()
    text = re.sub(r'[^\w\s\[\]]', '', text)  # ä¿ç•™è‡ªå®šç¾© token
    return text

train['text_clean'] = train['text'].astype(str).apply(clean_text)
test['text_clean'] = test['text'].astype(str).apply(clean_text)

# 6. åœ¨æ–‡å­—å°¾éƒ¨åŠ ä¸Š meta token
def add_meta_tokens(row):
    tokens = []

    if row['has_disaster_keyword']:
        tokens.append('[DISASTER_KEYWORD]')
    # è‹¥é‚„æœ‰å…¶ä»– featureï¼Œå¦‚ has_question, has_exclamationï¼Œå¯ä¸€ä½µåŠ ä¸Š

    return row['text_clean'] + ' ' + ' '.join(tokens)

train['text_clean'] = train.apply(add_meta_tokens, axis=1)
test['text_clean'] = test.apply(add_meta_tokens, axis=1)

# 7. è½‰æˆåˆ—è¡¨çµ¦ tokenizer ç”¨
train_texts = train['text_clean'].tolist()
train_labels = train['target'].tolist()
test_texts = test['text_clean'].tolist()