# -*- coding: utf-8 -*-
"""Final_code_0.84584.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifobVmvEBfZUV18iGOB2GNeXO8uuc8Kc
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 安裝必要套件
!pip install transformers
!pip install nltk
!pip install matplotlib
!pip install seaborn

# 匯入函式庫
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from torch.optim import AdamW
from transformers import get_scheduler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report
from tqdm import tqdm
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords

# 設定繪圖風格
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# 設定設備
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 載入資料
train = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/train.csv")
test = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/test.csv")
submission = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/sample_submission.csv")

# 使用 text 欄位與 target 作為輸入與標籤
train_texts = train['text'].astype(str).tolist()
train_labels = train['target'].tolist()
test_texts = test['text'].astype(str).tolist()

# 使用 RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-large')

# 定義 Dataset 類別
class TweetDataset(Dataset):
    def __init__(self, texts, labels=None, max_length=64, padding='max_length'):
        self.encodings = tokenizer(texts, truncation=True, padding=padding, max_length=max_length)
        self.labels = labels

    def __len__(self):
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx])
        return item

# 儲存每個 fold 的結果
fold_results = defaultdict(list)
fold_roc_curves = []

# Cross-validation + 模型平均
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
test_preds = np.zeros((len(test), 2))

for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):
    print(f"\n=== Fold {fold+1} ===")

    train_texts_fold = [train_texts[i] for i in train_idx]
    train_labels_fold = [train_labels[i] for i in train_idx]
    val_texts_fold = [train_texts[i] for i in val_idx]
    val_labels_fold = [train_labels[i] for i in val_idx]

    train_dataset = TweetDataset(train_texts_fold, train_labels_fold)
    val_dataset = TweetDataset(val_texts_fold, val_labels_fold)
    test_dataset = TweetDataset(test_texts)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8)
    test_loader = DataLoader(test_dataset, batch_size=8)

    model = RobertaForSequenceClassification.from_pretrained("roberta-large", num_labels=2)
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=2e-5)
    num_training_steps = len(train_loader) * 3
    lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0,
                                 num_training_steps=num_training_steps)

    # 儲存訓練歷史
    train_losses = []
    val_accuracies = []
    val_f1_scores = []

    # 訓練
    for epoch in range(3):
        print(f"Epoch {epoch + 1}")
        model.train()
        epoch_losses = []

        loop = tqdm(train_loader, leave=True)
        for batch in loop:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            epoch_losses.append(loss.item())
            loop.set_postfix(loss=loss.item())

        avg_train_loss = np.mean(epoch_losses)
        train_losses.append(avg_train_loss)

        # 驗證
        model.eval()
        val_preds = []
        val_probs = []
        val_true = []

        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**batch)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1).cpu().numpy()
                preds = np.argmax(probs, axis=1)

                val_preds.extend(preds)
                val_probs.extend(probs[:, 1])  # 正類別的機率
                val_true.extend(batch['labels'].cpu().numpy())

        # 計算指標
        val_accuracy = accuracy_score(val_true, val_preds)
        val_f1 = f1_score(val_true, val_preds, average='weighted')
        val_accuracies.append(val_accuracy)
        val_f1_scores.append(val_f1)

        print(f"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}")

    # 最終驗證評估
    model.eval()
    final_val_preds = []
    final_val_probs = []
    final_val_true = []

    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1).cpu().numpy()
            preds = np.argmax(probs, axis=1)

            final_val_preds.extend(preds)
            final_val_probs.extend(probs[:, 1])
            final_val_true.extend(batch['labels'].cpu().numpy())

    # 計算最終指標
    final_accuracy = accuracy_score(final_val_true, final_val_preds)
    final_f1 = f1_score(final_val_true, final_val_preds, average='weighted')
    final_roc_auc = roc_auc_score(final_val_true, final_val_probs)

    # 儲存結果
    fold_results['accuracy'].append(final_accuracy)
    fold_results['f1'].append(final_f1)
    fold_results['roc_auc'].append(final_roc_auc)
    fold_results['train_losses'].append(train_losses)
    fold_results['val_accuracies'].append(val_accuracies)
    fold_results['val_f1_scores'].append(val_f1_scores)

    # 儲存 ROC 曲線資料
    fpr, tpr, _ = roc_curve(final_val_true, final_val_probs)
    fold_roc_curves.append((fpr, tpr, final_roc_auc))

    # 儲存混淆矩陣
    cm = confusion_matrix(final_val_true, final_val_preds)
    fold_results['confusion_matrices'].append(cm)

    print(f"Fold {fold+1} Results:")
    print(f"Accuracy: {final_accuracy:.4f}")
    print(f"F1 Score: {final_f1:.4f}")
    print(f"ROC AUC: {final_roc_auc:.4f}")
    print(f"Classification Report:\n{classification_report(final_val_true, final_val_preds)}")

    # 預測 test
    fold_preds = []
    with torch.no_grad():
        for batch in test_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1).cpu().numpy()
            fold_preds.append(probs)

    fold_preds = np.vstack(fold_preds)
    test_preds += fold_preds / 5

# 最終預測
final_preds = np.argmax(test_preds, axis=1)
submission['target'] = final_preds
submission.to_csv("submission_cv_ensemble.csv", index=False)
print("✅ submission_cv_ensemble.csv 儲存完成")

# 繪製結果圖表
def plot_results():
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('5-Fold Cross-Validation Results', fontsize=16, fontweight='bold')

    # 1. 準確率分布
    axes[0, 0].boxplot(fold_results['accuracy'])
    axes[0, 0].set_title('Accuracy Distribution')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_xticklabels(['Accuracy'])
    axes[0, 0].grid(True, alpha=0.3)

    # 添加平均值和標準差
    mean_acc = np.mean(fold_results['accuracy'])
    std_acc = np.std(fold_results['accuracy'])
    axes[0, 0].text(0.5, 0.95, f'Mean: {mean_acc:.4f}\nStd: {std_acc:.4f}',
                    transform=axes[0, 0].transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # 2. F1 分數分布
    axes[0, 1].boxplot(fold_results['f1'])
    axes[0, 1].set_title('F1 Score Distribution')
    axes[0, 1].set_ylabel('F1 Score')
    axes[0, 1].set_xticklabels(['F1 Score'])
    axes[0, 1].grid(True, alpha=0.3)

    mean_f1 = np.mean(fold_results['f1'])
    std_f1 = np.std(fold_results['f1'])
    axes[0, 1].text(0.5, 0.95, f'Mean: {mean_f1:.4f}\nStd: {std_f1:.4f}',
                    transform=axes[0, 1].transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # 3. ROC AUC 分布
    axes[0, 2].boxplot(fold_results['roc_auc'])
    axes[0, 2].set_title('ROC AUC Distribution')
    axes[0, 2].set_ylabel('ROC AUC')
    axes[0, 2].set_xticklabels(['ROC AUC'])
    axes[0, 2].grid(True, alpha=0.3)

    mean_auc = np.mean(fold_results['roc_auc'])
    std_auc = np.std(fold_results['roc_auc'])
    axes[0, 2].text(0.5, 0.95, f'Mean: {mean_auc:.4f}\nStd: {std_auc:.4f}',
                    transform=axes[0, 2].transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # 4. ROC 曲線
    colors = ['blue', 'red', 'green', 'purple', 'orange']
    for i, (fpr, tpr, auc) in enumerate(fold_roc_curves):
        axes[1, 0].plot(fpr, tpr, color=colors[i], alpha=0.7,
                       label=f'Fold {i+1} (AUC = {auc:.3f})')

    axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
    axes[1, 0].set_xlabel('False Positive Rate')
    axes[1, 0].set_ylabel('True Positive Rate')
    axes[1, 0].set_title('ROC Curves for All Folds')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # 5. 訓練損失曲線
    for i, losses in enumerate(fold_results['train_losses']):
        axes[1, 1].plot(range(1, len(losses)+1), losses,
                       color=colors[i], alpha=0.7, label=f'Fold {i+1}')

    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Training Loss')
    axes[1, 1].set_title('Training Loss Curves')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # 6. 驗證準確率曲線
    for i, acc in enumerate(fold_results['val_accuracies']):
        axes[1, 2].plot(range(1, len(acc)+1), acc,
                       color=colors[i], alpha=0.7, label=f'Fold {i+1}')

    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Validation Accuracy')
    axes[1, 2].set_title('Validation Accuracy Curves')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('cv_results.png', dpi=300, bbox_inches='tight')
    plt.show()

# 繪製混淆矩陣
def plot_confusion_matrices():
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Confusion Matrices for All Folds', fontsize=16, fontweight='bold')

    for i, cm in enumerate(fold_results['confusion_matrices']):
        row = i // 3
        col = i % 3

        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   ax=axes[row, col], cbar=False)
        axes[row, col].set_title(f'Fold {i+1}')
        axes[row, col].set_xlabel('Predicted')
        axes[row, col].set_ylabel('Actual')

    # 移除最後一個空的子圖
    axes[1, 2].remove()

    plt.tight_layout()
    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')
    plt.show()

# 繪製指標比較圖
def plot_metrics_comparison():
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))

    folds = [f'Fold {i+1}' for i in range(5)]
    x = np.arange(len(folds))
    width = 0.25

    ax.bar(x - width, fold_results['accuracy'], width, label='Accuracy', alpha=0.8)
    ax.bar(x, fold_results['f1'], width, label='F1 Score', alpha=0.8)
    ax.bar(x + width, fold_results['roc_auc'], width, label='ROC AUC', alpha=0.8)

    ax.set_xlabel('Folds')
    ax.set_ylabel('Score')
    ax.set_title('Performance Metrics Comparison Across Folds')
    ax.set_xticks(x)
    ax.set_xticklabels(folds)
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 添加數值標籤
    for i, fold in enumerate(folds):
        ax.text(i - width, fold_results['accuracy'][i] + 0.01,
               f'{fold_results["accuracy"][i]:.3f}', ha='center', va='bottom')
        ax.text(i, fold_results['f1'][i] + 0.01,
               f'{fold_results["f1"][i]:.3f}', ha='center', va='bottom')
        ax.text(i + width, fold_results['roc_auc'][i] + 0.01,
               f'{fold_results["roc_auc"][i]:.3f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# 繪製所有圖表
print("\n" + "="*50)
print("繪製評估結果圖表...")
print("="*50)

plot_results()
plot_confusion_matrices()
plot_metrics_comparison()

# 輸出最終統計結果
print("\n" + "="*50)
print("最終交叉驗證結果統計")
print("="*50)
print(f"平均準確率: {np.mean(fold_results['accuracy']):.4f} ± {np.std(fold_results['accuracy']):.4f}")
print(f"平均 F1 分數: {np.mean(fold_results['f1']):.4f} ± {np.std(fold_results['f1']):.4f}")
print(f"平均 ROC AUC: {np.mean(fold_results['roc_auc']):.4f} ± {np.std(fold_results['roc_auc']):.4f}")
print("="*50)