{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4D7Ng_0l1DMs","executionInfo":{"status":"ok","timestamp":1746538904401,"user_tz":-480,"elapsed":29369,"user":{"displayName":"劉姮廷","userId":"05854787880116021429"}},"outputId":"6f85d529-a62b-45c6-d979-8dfb25cde0f1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-SPPHL3CB1o","outputId":"59b76d06-eba8-4632-aa43-9bfef496dd65","executionInfo":{"status":"ok","timestamp":1746539702533,"user_tz":-480,"elapsed":326205,"user":{"displayName":"劉姮廷","userId":"05854787880116021429"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 0: 100%|██████████| 429/429 [01:41<00:00,  4.24it/s, loss=0.2]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 429/429 [01:41<00:00,  4.25it/s, loss=0.0768]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 429/429 [01:41<00:00,  4.25it/s, loss=0.233]\n","100%|██████████| 204/204 [00:13<00:00, 15.54it/s]"]},{"output_type":"stream","name":"stdout","text":["✅ submission.csv 已儲存完成\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# 安裝必要套件\n","!pip install transformers\n","!pip install nltk\n","\n","# 匯入函式庫\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.optim import AdamW\n","\n","from transformers import get_scheduler\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","\n","# 設定設備\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 載入資料\n","train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/train.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/test.csv\")\n","submission = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/sample_submission.csv\")\n","\n","# 使用 text 欄位與 target 作為輸入與標籤\n","train_texts = train['text'].astype(str).tolist()\n","train_labels = train['target'].tolist()\n","test_texts = test['text'].astype(str).tolist()\n","\n","# 使用 BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 定義 Dataset 類別\n","class TweetDataset(Dataset):\n","    def __init__(self, texts, labels=None):\n","        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.encodings['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels is not None:\n","            item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","# 分割驗證集\n","train_texts_split, val_texts, train_labels_split, val_labels = train_test_split(\n","    train_texts, train_labels, test_size=0.1, random_state=42)\n","\n","# 建立 dataset\n","train_dataset = TweetDataset(train_texts_split, train_labels_split)\n","val_dataset = TweetDataset(val_texts, val_labels)\n","test_dataset = TweetDataset(test_texts)\n","\n","# 建立 dataloader\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16)\n","test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","# 載入 BERT 模型\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","model.to(device)\n","\n","# 設定 optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# 設定 scheduler\n","num_training_steps = len(train_loader) * 3  # 3 epochs\n","lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0,\n","                             num_training_steps=num_training_steps)\n","\n","# 訓練模型\n","model.train()\n","for epoch in range(3):\n","    print(f\"Epoch {epoch + 1}\")\n","    loop = tqdm(train_loader, leave=True)\n","    for batch in loop:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        loop.set_description(f\"Epoch {epoch}\")\n","        loop.set_postfix(loss=loss.item())\n","\n","# 預測 test set\n","model.eval()\n","predictions = []\n","with torch.no_grad():\n","    for batch in tqdm(test_loader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        preds = torch.argmax(outputs.logits, dim=-1)\n","        predictions.extend(preds.cpu().numpy())\n","\n","# 生成 submission 檔案\n","submission['target'] = predictions\n","submission.to_csv(\"/content/drive/MyDrive/Colab Notebooks/data/submission.csv\", index=False)\n","print(\"✅ submission.csv 已儲存完成\")\n"]}]}